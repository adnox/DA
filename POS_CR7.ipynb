{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DI9XhBAb34pY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f565014e-7094-45cf-e3d9-75c537ef9192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input text\n",
        "txt=open('/content/CR7.txt','r').read()"
      ],
      "metadata": {
        "id": "Ez0t6bYa4mw8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing data\n",
        "nltk.download('punkt')\n",
        "tokenized_data = sent_tokenize(txt)"
      ],
      "metadata": {
        "id": "oBYLwrBG48E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c0d2bc-a99d-4d2e-d844-a8a3a8e9528e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopword removal and pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "for i in tokenized_data:\n",
        "   words = nltk.word_tokenize(i)\n",
        "   words = [i for i in words if not i in stop_words]\n",
        "   tagged_data = nltk.pos_tag(words)\n",
        "   print(tagged_data)"
      ],
      "metadata": {
        "id": "ShVpSB6O5Ct3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47d63ad-a6df-4238-8b18-5e985f4244a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('returned', 'VBD'), ('Manchester', 'NNP'), ('United', 'NNP'), ('two', 'CD'), ('reasons', 'NNS'), ('reported', 'VBD'), ('said', 'VBD'), ('.', '.')]\n",
            "[('The', 'DT'), ('first', 'JJ'), ('I', 'PRP'), ('love', 'VBP'), ('club', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('second', 'JJ'), ('I', 'PRP'), ('love', 'VBP'), ('winning', 'VBG'), ('mentality', 'NN'), ('breeds', 'NNS'), ('ranks', 'VBZ'), ('club', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('come', 'VBP'), ('back', 'RB'), ('cheerleader', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('guys', 'NNS'), ('want', 'VBP'), ('succeed', 'VB'), (',', ','), ('I', 'PRP'), ('need', 'VBP'), ('love', 'VB'), ('club', 'NN'), ('bottom', 'NN'), ('hearts', 'NNS'), ('.', '.')]\n",
            "[('You', 'PRP'), ('need', 'VBP'), ('eat', 'VB'), (',', ','), ('sleep', 'VB'), ('fight', 'NN'), ('club', 'NN'), ('.', '.')]\n",
            "[('Whether', 'NNP'), ('play', 'NN'), ('play', 'NN'), (',', ','), ('need', 'VBP'), ('support', 'NN'), ('teammates', 'NNS'), ('always', 'RB'), ('give', 'VBP'), ('100', 'CD'), ('per', 'IN'), ('cent', 'NN'), ('club', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('win', 'VBP'), ('nothing', 'NN'), ('else', 'RB'), ('.', '.')]\n",
            "[('Winning', 'VBG'), ('brings', 'NNS'), ('us', 'PRP'), ('happiness', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting noun phrases \n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(txt)\n"
      ],
      "metadata": {
        "id": "Mw1TaOi46EJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing noun phrases\n",
        "for np in doc.noun_chunks:\n",
        "      print(np)\n"
      ],
      "metadata": {
        "id": "CnL9foFq5WCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7295066-3b86-4ea2-e155-bb99f33172e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "Manchester United\n",
            "two reasons\n",
            "he\n",
            "I\n",
            "the club\n",
            "I\n",
            "the winning mentality\n",
            "that\n",
            "the ranks\n",
            "this club\n",
            "I\n",
            "a cheerleader\n",
            "you guys\n",
            "I\n",
            "you\n",
            "this club\n",
            "the bottom\n",
            "your hearts\n",
            "You\n",
            "this club\n",
            "you\n",
            "you\n",
            "your teammates\n",
            "100 per cent\n",
            "the club\n",
            "I\n",
            "nothing\n",
            "us\n",
            "happiness\n"
          ]
        }
      ]
    }
  ]
}